{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumeet/chess-deep-learning/blob/main/chess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARzA9g1bNbzS"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0UbN2mBMGlz"
      },
      "outputs": [],
      "source": [
        "#!wget \"https://database.lichess.org/standard/lichess_db_standard_rated_2017-02.pgn.zst\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stlNZAO7BtNX"
      },
      "outputs": [],
      "source": [
        "#!apt-get update && apt-get install zstd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3spk0B8tCZxj",
        "outputId": "19a15575-e6cd-49d3-b3f3-4ca8943d632b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lichess_db_standard_rated_2017-02.pgn.zst: 9289131923 bytes \n"
          ]
        }
      ],
      "source": [
        "#!unzstd *.zst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN_b6O-CNDbs"
      },
      "source": [
        "# Input and outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amM2jZBeELgS",
        "outputId": "8d248096-a24c-4518-ac84-e8cfaa726770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-chess\n",
            "  Downloading python_chess-1.999-py3-none-any.whl (1.4 kB)\n",
            "Collecting chess<2,>=1 (from python-chess)\n",
            "  Downloading chess-1.9.4-py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: chess, python-chess\n",
            "Successfully installed chess-1.9.4 python-chess-1.999\n",
            "processed 1000 games already\n",
            "processed 2000 games already\n",
            "processed 3000 games already\n",
            "processed 4000 games already\n",
            "processed 5000 games already\n",
            "processed 6000 games already\n",
            "processed 7000 games already\n",
            "processed 8000 games already\n",
            "processed 9000 games already\n",
            "processed 10000 games already\n",
            "processed 11000 games already\n",
            "processed 12000 games already\n",
            "processed 13000 games already\n",
            "processed 14000 games already\n",
            "processed 15000 games already\n",
            "processed 16000 games already\n",
            "processed 17000 games already\n",
            "processed 18000 games already\n",
            "processed 19000 games already\n",
            "processed 20000 games already\n",
            "processed 21000 games already\n",
            "loaded 773032 input  total\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from chess import pgn\n",
        "except ImportError:    \n",
        "    !pip install python-chess\n",
        "    import chess\n",
        "    from chess import pgn\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "f = open('./drive/MyDrive/MLchess/elite.chess')\n",
        "\n",
        "def board_to_tensor(board):\n",
        "    pieces_order = ['pawn', 'knight', 'bishop', 'rook', 'queen', 'king']  # Chess piece types\n",
        "    pieces = {name: i for i, name in enumerate(pieces_order)}\n",
        "    tensor = torch.zeros((8, 8, 12), dtype=torch.float32)\n",
        "\n",
        "    # Loop through all squares on the board\n",
        "    for square in chess.SQUARES:\n",
        "        piece = board.piece_at(square)\n",
        "        if piece:\n",
        "            # Get piece type and color\n",
        "            piece_type = chess.piece_name(piece.piece_type)\n",
        "            color = piece.color\n",
        "\n",
        "            # Calculate the coordinates\n",
        "            row = square // 8\n",
        "            col = square % 8\n",
        "\n",
        "            # Calculate the depth index based on the piece type and color\n",
        "            piece_index = pieces[piece_type] + (6 if color == chess.BLACK else 0)\n",
        "\n",
        "            # Place the piece in the tensor\n",
        "            tensor[row, col, piece_index] = 1\n",
        "\n",
        "    # Flatten the tensor\n",
        "    tensor = tensor.view(-1)\n",
        "\n",
        "    # Append castling rights\n",
        "    tensor = torch.cat((tensor, torch.tensor([\n",
        "        int(board.has_kingside_castling_rights(chess.WHITE)),\n",
        "        int(board.has_queenside_castling_rights(chess.WHITE)),\n",
        "        int(board.has_kingside_castling_rights(chess.BLACK)),\n",
        "        int(board.has_queenside_castling_rights(chess.BLACK)),\n",
        "    ])))\n",
        "\n",
        "    # Append en passant square\n",
        "    if board.ep_square is not None:\n",
        "        tensor = torch.cat((tensor, torch.tensor([board.ep_square])))\n",
        "    else:\n",
        "        tensor = torch.cat((tensor, torch.tensor([64])))  # Append 64 if there is no en passant square\n",
        "\n",
        "    # Append white's turn\n",
        "    tensor = torch.cat((tensor, torch.tensor([1 if board.turn == chess.WHITE else 0])))\n",
        "\n",
        "    # Append is in check\n",
        "    tensor = torch.cat((tensor, torch.tensor([0 if board.is_check() else -1])))\n",
        "\n",
        "    return tensor\n",
        "\n",
        "cache = {}\n",
        "reverse_cache = {}\n",
        "for i, src_file in enumerate('abcdefgh'):\n",
        "    for j, src_rank in enumerate('12345678'):\n",
        "        for k, dst_file in enumerate('abcdefgh'):\n",
        "            for l, dst_rank in enumerate('12345678'):\n",
        "                src_matrix = i*8+j\n",
        "                dst_matrix = k*8+l\n",
        "                src_move = src_file + src_rank \n",
        "                dst_move = dst_file + dst_rank\n",
        "                reverse_cache[src_matrix] = src_move\n",
        "                reverse_cache[dst_matrix] = dst_move\n",
        "                cache[src_move + dst_move] = (\n",
        "                    tensor(src_matrix, dtype=torch.long),\n",
        "                    tensor(dst_matrix, dtype=torch.long))\n",
        "\n",
        "def move_to_tensors(chess_move):\n",
        "    return cache[chess_move[:4]]\n",
        "\n",
        "\n",
        "input_tensors = []\n",
        "output_src_tensors = []\n",
        "output_dst_tensors = []\n",
        "num_games = 0\n",
        "\n",
        "for line in f:\n",
        "    moves = line.split(\",\")\n",
        "    winner = next(f).strip()\n",
        "    if winner == \"D\":\n",
        "        continue\n",
        "\n",
        "    to_move = \"W\"\n",
        "\n",
        "    board = chess.Board()\n",
        "    for (i, move) in enumerate(moves):\n",
        "        # (was made by winner)\n",
        "        use_as_input = to_move == winner and i < len(moves)\n",
        "\n",
        "        if use_as_input:\n",
        "            input_tensors.append(board_to_tensor(board))\n",
        "\n",
        "        move = move.strip()\n",
        "        board.push_san(move)\n",
        "\n",
        "        if use_as_input:\n",
        "            (src, dst) = move_to_tensors(move)\n",
        "            output_src_tensors.append(src)\n",
        "            output_dst_tensors.append(dst)\n",
        "\n",
        "        to_move = \"W\" if to_move == \"B\" else \"B\"\n",
        "\n",
        "    num_games += 1\n",
        "\n",
        "    if num_games % 1000 == 0:\n",
        "        print(f'processed {num_games} games already')\n",
        "\n",
        "    if num_games > 100_000:\n",
        "        break\n",
        "\n",
        "\n",
        "print(f'loaded {len(input_tensors)} input  total')\n",
        "\n",
        "    \n",
        "training_size = 50_000\n",
        "training_size = len(input_tensors)\n",
        "input_batch = torch.stack(input_tensors[:training_size])\n",
        "output_src_batch = torch.stack(output_src_tensors[:training_size])\n",
        "output_dst_batch = torch.stack(output_dst_tensors[:training_size])\n",
        "\n",
        "# test_input = torch.stack(input_sequences[training_size:])\n",
        "# test_output = torch.stack(output_sequences[training_size:])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAC1-ip8plVq",
        "outputId": "fc9e452a-5466-40bc-ff52-d611d713985a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ1Jz5pINLyL"
      },
      "source": [
        "# Define Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9K2dhq_NPRf"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "class ChessMovePredictor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_squares):\n",
        "        super(ChessMovePredictor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc_source = nn.Linear(hidden_dim, num_squares)\n",
        "        self.fc_dest = nn.Linear(hidden_dim, num_squares)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.fc1(x))\n",
        "        out = self.relu(self.fc2(out))\n",
        "        out = self.relu(self.fc3(out))\n",
        "        out = self.relu(self.fc4(out))\n",
        "        out_source = self.fc_source(out)\n",
        "        out_dest = self.fc_dest(out)\n",
        "        return out_source, out_dest\n",
        "\n",
        "\n",
        "input_dim = 775\n",
        "hidden_dim = 512\n",
        "num_squares = 64\n",
        "model = ChessMovePredictor(input_dim, hidden_dim, num_squares)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSbk7JsAP3CZ"
      },
      "source": [
        "# Train Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C4fX2R_SP4av",
        "outputId": "f5682551-6b85-446b-88eb-7cd12ddc6e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [1/10000], Loss: 5.8323\n",
            "Epoch [2/10000], Loss: 6.0742\n",
            "Epoch [3/10000], Loss: 5.1638\n",
            "Epoch [4/10000], Loss: 5.4865\n",
            "Epoch [5/10000], Loss: 5.0492\n",
            "Epoch [6/10000], Loss: 5.1270\n",
            "Epoch [7/10000], Loss: 5.3261\n",
            "Epoch [8/10000], Loss: 4.2962\n",
            "Epoch [9/10000], Loss: 4.9046\n",
            "Epoch [10/10000], Loss: 4.7335\n",
            "Epoch [11/10000], Loss: 4.4557\n",
            "Epoch [12/10000], Loss: 3.6573\n",
            "Epoch [13/10000], Loss: 4.1042\n",
            "Epoch [14/10000], Loss: 3.8259\n",
            "Epoch [15/10000], Loss: 4.2100\n",
            "Epoch [16/10000], Loss: 5.2944\n",
            "Epoch [17/10000], Loss: 4.5074\n",
            "Epoch [18/10000], Loss: 4.5068\n",
            "Epoch [19/10000], Loss: 4.1979\n",
            "Epoch [20/10000], Loss: 3.5514\n",
            "Epoch [21/10000], Loss: 3.9322\n",
            "Epoch [22/10000], Loss: 3.7583\n",
            "Epoch [23/10000], Loss: 3.4434\n",
            "Epoch [24/10000], Loss: 4.1425\n",
            "Epoch [25/10000], Loss: 4.2355\n",
            "Epoch [26/10000], Loss: 4.0356\n",
            "Epoch [27/10000], Loss: 4.4893\n",
            "Epoch [28/10000], Loss: 4.0994\n",
            "Epoch [29/10000], Loss: 3.4036\n",
            "Epoch [30/10000], Loss: 3.8348\n",
            "Epoch [31/10000], Loss: 3.6065\n",
            "Epoch [32/10000], Loss: 3.5816\n",
            "Epoch [33/10000], Loss: 3.8874\n",
            "Epoch [34/10000], Loss: 3.5957\n",
            "Epoch [35/10000], Loss: 3.2562\n",
            "Epoch [36/10000], Loss: 3.6722\n",
            "Epoch [37/10000], Loss: 3.9191\n",
            "Epoch [38/10000], Loss: 3.1951\n",
            "Epoch [39/10000], Loss: 3.4413\n",
            "Epoch [40/10000], Loss: 3.0675\n",
            "Epoch [41/10000], Loss: 4.0358\n",
            "Epoch [42/10000], Loss: 3.6716\n",
            "Epoch [43/10000], Loss: 3.7619\n",
            "Epoch [44/10000], Loss: 3.5344\n",
            "Epoch [45/10000], Loss: 3.9635\n",
            "Epoch [46/10000], Loss: 3.9283\n",
            "Epoch [47/10000], Loss: 3.4274\n",
            "Epoch [48/10000], Loss: 3.7789\n",
            "Epoch [49/10000], Loss: 3.8658\n",
            "Epoch [50/10000], Loss: 3.5928\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [51/10000], Loss: 4.4032\n",
            "Epoch [52/10000], Loss: 4.1532\n",
            "Epoch [53/10000], Loss: 3.8534\n",
            "Epoch [54/10000], Loss: 3.8128\n",
            "Epoch [55/10000], Loss: 3.8965\n",
            "Epoch [56/10000], Loss: 4.2309\n",
            "Epoch [57/10000], Loss: 3.4674\n",
            "Epoch [58/10000], Loss: 3.5255\n",
            "Epoch [59/10000], Loss: 4.1458\n",
            "Epoch [60/10000], Loss: 3.8300\n",
            "Epoch [61/10000], Loss: 4.1333\n",
            "Epoch [62/10000], Loss: 3.6888\n",
            "Epoch [63/10000], Loss: 4.7814\n",
            "Epoch [64/10000], Loss: 3.2238\n",
            "Epoch [65/10000], Loss: 3.5329\n",
            "Epoch [66/10000], Loss: 3.8394\n",
            "Epoch [67/10000], Loss: 3.7817\n",
            "Epoch [68/10000], Loss: 4.1743\n",
            "Epoch [69/10000], Loss: 3.5948\n",
            "Epoch [70/10000], Loss: 3.8243\n",
            "Epoch [71/10000], Loss: 3.7171\n",
            "Epoch [72/10000], Loss: 2.9854\n",
            "Epoch [73/10000], Loss: 3.1337\n",
            "Epoch [74/10000], Loss: 3.8054\n",
            "Epoch [75/10000], Loss: 3.7319\n",
            "Epoch [76/10000], Loss: 4.0138\n",
            "Epoch [77/10000], Loss: 3.9726\n",
            "Epoch [78/10000], Loss: 3.3831\n",
            "Epoch [79/10000], Loss: 3.7974\n",
            "Epoch [80/10000], Loss: 3.1401\n",
            "Epoch [81/10000], Loss: 4.0214\n",
            "Epoch [82/10000], Loss: 3.8573\n",
            "Epoch [83/10000], Loss: 3.4494\n",
            "Epoch [84/10000], Loss: 3.5230\n",
            "Epoch [85/10000], Loss: 4.2256\n",
            "Epoch [86/10000], Loss: 3.2276\n",
            "Epoch [87/10000], Loss: 3.6949\n",
            "Epoch [88/10000], Loss: 3.5118\n",
            "Epoch [89/10000], Loss: 3.7997\n",
            "Epoch [90/10000], Loss: 2.9338\n",
            "Epoch [91/10000], Loss: 3.3208\n",
            "Epoch [92/10000], Loss: 3.8046\n",
            "Epoch [93/10000], Loss: 3.5344\n",
            "Epoch [94/10000], Loss: 3.6530\n",
            "Epoch [95/10000], Loss: 2.9432\n",
            "Epoch [96/10000], Loss: 3.7028\n",
            "Epoch [97/10000], Loss: 3.0735\n",
            "Epoch [98/10000], Loss: 3.9602\n",
            "Epoch [99/10000], Loss: 3.4459\n",
            "Epoch [100/10000], Loss: 3.2829\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [101/10000], Loss: 2.8308\n",
            "Epoch [102/10000], Loss: 3.8415\n",
            "Epoch [103/10000], Loss: 3.3514\n",
            "Epoch [104/10000], Loss: 2.8300\n",
            "Epoch [105/10000], Loss: 2.9513\n",
            "Epoch [106/10000], Loss: 3.3846\n",
            "Epoch [107/10000], Loss: 3.8160\n",
            "Epoch [108/10000], Loss: 3.6067\n",
            "Epoch [109/10000], Loss: 2.5950\n",
            "Epoch [110/10000], Loss: 4.4528\n",
            "Epoch [111/10000], Loss: 3.1556\n",
            "Epoch [112/10000], Loss: 2.9807\n",
            "Epoch [113/10000], Loss: 3.7807\n",
            "Epoch [114/10000], Loss: 3.8180\n",
            "Epoch [115/10000], Loss: 3.8950\n",
            "Epoch [116/10000], Loss: 3.2891\n",
            "Epoch [117/10000], Loss: 3.4102\n",
            "Epoch [118/10000], Loss: 3.3114\n",
            "Epoch [119/10000], Loss: 3.2112\n",
            "Epoch [120/10000], Loss: 2.4364\n",
            "Epoch [121/10000], Loss: 2.7765\n",
            "Epoch [122/10000], Loss: 3.7569\n",
            "Epoch [123/10000], Loss: 2.8472\n",
            "Epoch [124/10000], Loss: 3.1830\n",
            "Epoch [125/10000], Loss: 3.2239\n",
            "Epoch [126/10000], Loss: 3.3537\n",
            "Epoch [127/10000], Loss: 3.4241\n",
            "Epoch [128/10000], Loss: 3.3089\n",
            "Epoch [129/10000], Loss: 3.2182\n",
            "Epoch [130/10000], Loss: 4.3933\n",
            "Epoch [131/10000], Loss: 4.1301\n",
            "Epoch [132/10000], Loss: 3.5874\n",
            "Epoch [133/10000], Loss: 3.4046\n",
            "Epoch [134/10000], Loss: 3.0584\n",
            "Epoch [135/10000], Loss: 4.1406\n",
            "Epoch [136/10000], Loss: 3.2673\n",
            "Epoch [137/10000], Loss: 2.8897\n",
            "Epoch [138/10000], Loss: 3.8310\n",
            "Epoch [139/10000], Loss: 3.7429\n",
            "Epoch [140/10000], Loss: 3.3413\n",
            "Epoch [141/10000], Loss: 3.2637\n",
            "Epoch [142/10000], Loss: 3.2222\n",
            "Epoch [143/10000], Loss: 3.6328\n",
            "Epoch [144/10000], Loss: 3.2449\n",
            "Epoch [145/10000], Loss: 3.5870\n",
            "Epoch [146/10000], Loss: 3.7346\n",
            "Epoch [147/10000], Loss: 3.2901\n",
            "Epoch [148/10000], Loss: 2.9716\n",
            "Epoch [149/10000], Loss: 2.8582\n",
            "Epoch [150/10000], Loss: 2.5650\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [151/10000], Loss: 3.5617\n",
            "Epoch [152/10000], Loss: 2.8211\n",
            "Epoch [153/10000], Loss: 3.8543\n",
            "Epoch [154/10000], Loss: 3.2170\n",
            "Epoch [155/10000], Loss: 3.2206\n",
            "Epoch [156/10000], Loss: 2.7044\n",
            "Epoch [157/10000], Loss: 3.9409\n",
            "Epoch [158/10000], Loss: 3.7027\n",
            "Epoch [159/10000], Loss: 2.8861\n",
            "Epoch [160/10000], Loss: 3.4894\n",
            "Epoch [161/10000], Loss: 3.4655\n",
            "Epoch [162/10000], Loss: 3.1198\n",
            "Epoch [163/10000], Loss: 3.7130\n",
            "Epoch [164/10000], Loss: 3.1269\n",
            "Epoch [165/10000], Loss: 3.7742\n",
            "Epoch [166/10000], Loss: 3.5723\n",
            "Epoch [167/10000], Loss: 3.3169\n",
            "Epoch [168/10000], Loss: 3.4187\n",
            "Epoch [169/10000], Loss: 3.8208\n",
            "Epoch [170/10000], Loss: 3.8014\n",
            "Epoch [171/10000], Loss: 3.5521\n",
            "Epoch [172/10000], Loss: 3.4407\n",
            "Epoch [173/10000], Loss: 4.1578\n",
            "Epoch [174/10000], Loss: 3.3989\n",
            "Epoch [175/10000], Loss: 2.9390\n",
            "Epoch [176/10000], Loss: 3.6778\n",
            "Epoch [177/10000], Loss: 3.4758\n",
            "Epoch [178/10000], Loss: 2.7782\n",
            "Epoch [179/10000], Loss: 2.9035\n",
            "Epoch [180/10000], Loss: 2.7421\n",
            "Epoch [181/10000], Loss: 2.5780\n",
            "Epoch [182/10000], Loss: 3.3909\n",
            "Epoch [183/10000], Loss: 3.5419\n",
            "Epoch [184/10000], Loss: 2.9484\n",
            "Epoch [185/10000], Loss: 3.3604\n",
            "Epoch [186/10000], Loss: 3.3794\n",
            "Epoch [187/10000], Loss: 3.0777\n",
            "Epoch [188/10000], Loss: 3.2226\n",
            "Epoch [189/10000], Loss: 3.5474\n",
            "Epoch [190/10000], Loss: 4.1831\n",
            "Epoch [191/10000], Loss: 3.5456\n",
            "Epoch [192/10000], Loss: 3.9306\n",
            "Epoch [193/10000], Loss: 3.5717\n",
            "Epoch [194/10000], Loss: 3.5211\n",
            "Epoch [195/10000], Loss: 2.9307\n",
            "Epoch [196/10000], Loss: 3.4123\n",
            "Epoch [197/10000], Loss: 3.1633\n",
            "Epoch [198/10000], Loss: 3.3710\n",
            "Epoch [199/10000], Loss: 4.1410\n",
            "Epoch [200/10000], Loss: 3.0850\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [201/10000], Loss: 3.1432\n",
            "Epoch [202/10000], Loss: 4.1240\n",
            "Epoch [203/10000], Loss: 3.8338\n",
            "Epoch [204/10000], Loss: 2.4103\n",
            "Epoch [205/10000], Loss: 3.6388\n",
            "Epoch [206/10000], Loss: 4.2050\n",
            "Epoch [207/10000], Loss: 3.3801\n",
            "Epoch [208/10000], Loss: 3.4632\n",
            "Epoch [209/10000], Loss: 3.9380\n",
            "Epoch [210/10000], Loss: 3.1024\n",
            "Epoch [211/10000], Loss: 3.1401\n",
            "Epoch [212/10000], Loss: 3.1590\n",
            "Epoch [213/10000], Loss: 2.9479\n",
            "Epoch [214/10000], Loss: 3.7245\n",
            "Epoch [215/10000], Loss: 3.5504\n",
            "Epoch [216/10000], Loss: 3.0505\n",
            "Epoch [217/10000], Loss: 3.9350\n",
            "Epoch [218/10000], Loss: 3.2590\n",
            "Epoch [219/10000], Loss: 3.1246\n",
            "Epoch [220/10000], Loss: 2.7499\n",
            "Epoch [221/10000], Loss: 3.0246\n",
            "Epoch [222/10000], Loss: 3.9611\n",
            "Epoch [223/10000], Loss: 3.7406\n",
            "Epoch [224/10000], Loss: 2.6873\n",
            "Epoch [225/10000], Loss: 3.3254\n",
            "Epoch [226/10000], Loss: 3.0499\n",
            "Epoch [227/10000], Loss: 3.6161\n",
            "Epoch [228/10000], Loss: 3.2573\n",
            "Epoch [229/10000], Loss: 3.4062\n",
            "Epoch [230/10000], Loss: 3.7254\n",
            "Epoch [231/10000], Loss: 2.3984\n",
            "Epoch [232/10000], Loss: 3.9316\n",
            "Epoch [233/10000], Loss: 3.0093\n",
            "Epoch [234/10000], Loss: 2.8778\n",
            "Epoch [235/10000], Loss: 3.5315\n",
            "Epoch [236/10000], Loss: 3.2622\n",
            "Epoch [237/10000], Loss: 2.6569\n",
            "Epoch [238/10000], Loss: 2.7353\n",
            "Epoch [239/10000], Loss: 3.7173\n",
            "Epoch [240/10000], Loss: 3.6904\n",
            "Epoch [241/10000], Loss: 3.2194\n",
            "Epoch [242/10000], Loss: 3.6955\n",
            "Epoch [243/10000], Loss: 3.3860\n",
            "Epoch [244/10000], Loss: 3.1680\n",
            "Epoch [245/10000], Loss: 4.1921\n",
            "Epoch [246/10000], Loss: 3.9428\n",
            "Epoch [247/10000], Loss: 3.0877\n",
            "Epoch [248/10000], Loss: 3.1860\n",
            "Epoch [249/10000], Loss: 3.3031\n",
            "Epoch [250/10000], Loss: 3.1619\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [251/10000], Loss: 3.6376\n",
            "Epoch [252/10000], Loss: 3.1960\n",
            "Epoch [253/10000], Loss: 3.6453\n",
            "Epoch [254/10000], Loss: 3.0831\n",
            "Epoch [255/10000], Loss: 3.1820\n",
            "Epoch [256/10000], Loss: 3.6380\n",
            "Epoch [257/10000], Loss: 2.7861\n",
            "Epoch [258/10000], Loss: 3.4024\n",
            "Epoch [259/10000], Loss: 2.7503\n",
            "Epoch [260/10000], Loss: 3.1907\n",
            "Epoch [261/10000], Loss: 3.5107\n",
            "Epoch [262/10000], Loss: 3.6048\n",
            "Epoch [263/10000], Loss: 3.0780\n",
            "Epoch [264/10000], Loss: 3.2664\n",
            "Epoch [265/10000], Loss: 3.5789\n",
            "Epoch [266/10000], Loss: 3.7694\n",
            "Epoch [267/10000], Loss: 2.8574\n",
            "Epoch [268/10000], Loss: 2.6263\n",
            "Epoch [269/10000], Loss: 3.1969\n",
            "Epoch [270/10000], Loss: 3.7410\n",
            "Epoch [271/10000], Loss: 3.8848\n",
            "Epoch [272/10000], Loss: 3.6612\n",
            "Epoch [273/10000], Loss: 4.0136\n",
            "Epoch [274/10000], Loss: 2.7910\n",
            "Epoch [275/10000], Loss: 3.3057\n",
            "Epoch [276/10000], Loss: 2.7644\n",
            "Epoch [277/10000], Loss: 3.0697\n",
            "Epoch [278/10000], Loss: 3.4794\n",
            "Epoch [279/10000], Loss: 3.6475\n",
            "Epoch [280/10000], Loss: 3.4703\n",
            "Epoch [281/10000], Loss: 3.0265\n",
            "Epoch [282/10000], Loss: 3.2993\n",
            "Epoch [283/10000], Loss: 2.7307\n",
            "Epoch [284/10000], Loss: 2.9533\n",
            "Epoch [285/10000], Loss: 4.0518\n",
            "Epoch [286/10000], Loss: 2.9307\n",
            "Epoch [287/10000], Loss: 2.9876\n",
            "Epoch [288/10000], Loss: 3.1838\n",
            "Epoch [289/10000], Loss: 2.8004\n",
            "Epoch [290/10000], Loss: 3.2582\n",
            "Epoch [291/10000], Loss: 2.5433\n",
            "Epoch [292/10000], Loss: 4.0312\n",
            "Epoch [293/10000], Loss: 3.3123\n",
            "Epoch [294/10000], Loss: 2.9693\n",
            "Epoch [295/10000], Loss: 2.9998\n",
            "Epoch [296/10000], Loss: 3.5137\n",
            "Epoch [297/10000], Loss: 3.8809\n",
            "Epoch [298/10000], Loss: 3.2829\n",
            "Epoch [299/10000], Loss: 2.9522\n",
            "Epoch [300/10000], Loss: 3.4483\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [301/10000], Loss: 3.2819\n",
            "Epoch [302/10000], Loss: 3.9309\n",
            "Epoch [303/10000], Loss: 3.3043\n",
            "Epoch [304/10000], Loss: 3.2826\n",
            "Epoch [305/10000], Loss: 3.8560\n",
            "Epoch [306/10000], Loss: 2.8531\n",
            "Epoch [307/10000], Loss: 3.4351\n",
            "Epoch [308/10000], Loss: 3.1906\n",
            "Epoch [309/10000], Loss: 2.7110\n",
            "Epoch [310/10000], Loss: 3.8267\n",
            "Epoch [311/10000], Loss: 3.3619\n",
            "Epoch [312/10000], Loss: 3.6235\n",
            "Epoch [313/10000], Loss: 3.2850\n",
            "Epoch [314/10000], Loss: 3.0268\n",
            "Epoch [315/10000], Loss: 2.7563\n",
            "Epoch [316/10000], Loss: 3.5624\n",
            "Epoch [317/10000], Loss: 2.8289\n",
            "Epoch [318/10000], Loss: 3.3922\n",
            "Epoch [319/10000], Loss: 3.7916\n",
            "Epoch [320/10000], Loss: 3.7340\n",
            "Epoch [321/10000], Loss: 3.9943\n",
            "Epoch [322/10000], Loss: 3.4961\n",
            "Epoch [323/10000], Loss: 2.7155\n",
            "Epoch [324/10000], Loss: 3.2311\n",
            "Epoch [325/10000], Loss: 3.3079\n",
            "Epoch [326/10000], Loss: 3.5853\n",
            "Epoch [327/10000], Loss: 3.7927\n",
            "Epoch [328/10000], Loss: 2.7847\n",
            "Epoch [329/10000], Loss: 3.7795\n",
            "Epoch [330/10000], Loss: 2.5857\n",
            "Epoch [331/10000], Loss: 3.3424\n",
            "Epoch [332/10000], Loss: 2.3686\n",
            "Epoch [333/10000], Loss: 2.6025\n",
            "Epoch [334/10000], Loss: 3.1909\n",
            "Epoch [335/10000], Loss: 3.8786\n",
            "Epoch [336/10000], Loss: 3.4276\n",
            "Epoch [337/10000], Loss: 3.2085\n",
            "Epoch [338/10000], Loss: 4.0984\n",
            "Epoch [339/10000], Loss: 3.4134\n",
            "Epoch [340/10000], Loss: 3.2668\n",
            "Epoch [341/10000], Loss: 2.9959\n",
            "Epoch [342/10000], Loss: 3.9734\n",
            "Epoch [343/10000], Loss: 3.3473\n",
            "Epoch [344/10000], Loss: 3.1830\n",
            "Epoch [345/10000], Loss: 3.6689\n",
            "Epoch [346/10000], Loss: 3.0649\n",
            "Epoch [347/10000], Loss: 3.6173\n",
            "Epoch [348/10000], Loss: 3.6540\n",
            "Epoch [349/10000], Loss: 3.4187\n",
            "Epoch [350/10000], Loss: 4.3052\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [351/10000], Loss: 3.5449\n",
            "Epoch [352/10000], Loss: 2.8900\n",
            "Epoch [353/10000], Loss: 3.5211\n",
            "Epoch [354/10000], Loss: 3.5092\n",
            "Epoch [355/10000], Loss: 3.2587\n",
            "Epoch [356/10000], Loss: 3.8103\n",
            "Epoch [357/10000], Loss: 3.7150\n",
            "Epoch [358/10000], Loss: 2.8463\n",
            "Epoch [359/10000], Loss: 3.1303\n",
            "Epoch [360/10000], Loss: 3.6636\n",
            "Epoch [361/10000], Loss: 2.7390\n",
            "Epoch [362/10000], Loss: 3.1840\n",
            "Epoch [363/10000], Loss: 3.5225\n",
            "Epoch [364/10000], Loss: 3.6832\n",
            "Epoch [365/10000], Loss: 3.5319\n",
            "Epoch [366/10000], Loss: 3.5733\n",
            "Epoch [367/10000], Loss: 2.6465\n",
            "Epoch [368/10000], Loss: 4.0063\n",
            "Epoch [369/10000], Loss: 2.7617\n",
            "Epoch [370/10000], Loss: 3.2987\n",
            "Epoch [371/10000], Loss: 3.0856\n",
            "Epoch [372/10000], Loss: 3.3988\n",
            "Epoch [373/10000], Loss: 3.4882\n",
            "Epoch [374/10000], Loss: 3.2746\n",
            "Epoch [375/10000], Loss: 2.8963\n",
            "Epoch [376/10000], Loss: 4.1093\n",
            "Epoch [377/10000], Loss: 3.4227\n",
            "Epoch [378/10000], Loss: 2.8328\n",
            "Epoch [379/10000], Loss: 2.7413\n",
            "Epoch [380/10000], Loss: 4.1499\n",
            "Epoch [381/10000], Loss: 3.4464\n",
            "Epoch [382/10000], Loss: 3.1300\n",
            "Epoch [383/10000], Loss: 3.5016\n",
            "Epoch [384/10000], Loss: 3.5138\n",
            "Epoch [385/10000], Loss: 2.2354\n",
            "Epoch [386/10000], Loss: 3.1724\n",
            "Epoch [387/10000], Loss: 3.2545\n",
            "Epoch [388/10000], Loss: 2.6438\n",
            "Epoch [389/10000], Loss: 2.6726\n",
            "Epoch [390/10000], Loss: 3.7242\n",
            "Epoch [391/10000], Loss: 2.7836\n",
            "Epoch [392/10000], Loss: 3.1016\n",
            "Epoch [393/10000], Loss: 3.5689\n",
            "Epoch [394/10000], Loss: 3.1784\n",
            "Epoch [395/10000], Loss: 3.4163\n",
            "Epoch [396/10000], Loss: 4.0187\n",
            "Epoch [397/10000], Loss: 2.6660\n",
            "Epoch [398/10000], Loss: 3.4500\n",
            "Epoch [399/10000], Loss: 2.4366\n",
            "Epoch [400/10000], Loss: 3.5329\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [401/10000], Loss: 3.0050\n",
            "Epoch [402/10000], Loss: 3.1731\n",
            "Epoch [403/10000], Loss: 3.2755\n",
            "Epoch [404/10000], Loss: 3.3448\n",
            "Epoch [405/10000], Loss: 3.2220\n",
            "Epoch [406/10000], Loss: 3.3284\n",
            "Epoch [407/10000], Loss: 2.7929\n",
            "Epoch [408/10000], Loss: 2.9987\n",
            "Epoch [409/10000], Loss: 3.1400\n",
            "Epoch [410/10000], Loss: 3.5772\n",
            "Epoch [411/10000], Loss: 2.7955\n",
            "Epoch [412/10000], Loss: 3.2113\n",
            "Epoch [413/10000], Loss: 3.3451\n",
            "Epoch [414/10000], Loss: 2.7170\n",
            "Epoch [415/10000], Loss: 3.2017\n",
            "Epoch [416/10000], Loss: 3.1270\n",
            "Epoch [417/10000], Loss: 3.2748\n",
            "Epoch [418/10000], Loss: 2.7796\n",
            "Epoch [419/10000], Loss: 3.0674\n",
            "Epoch [420/10000], Loss: 3.1293\n",
            "Epoch [421/10000], Loss: 2.8452\n",
            "Epoch [422/10000], Loss: 3.5705\n",
            "Epoch [423/10000], Loss: 3.0818\n",
            "Epoch [424/10000], Loss: 3.4760\n",
            "Epoch [425/10000], Loss: 3.3288\n",
            "Epoch [426/10000], Loss: 2.8194\n",
            "Epoch [427/10000], Loss: 3.9218\n",
            "Epoch [428/10000], Loss: 3.3161\n",
            "Epoch [429/10000], Loss: 3.8027\n",
            "Epoch [430/10000], Loss: 3.0654\n",
            "Epoch [431/10000], Loss: 2.9232\n",
            "Epoch [432/10000], Loss: 3.1871\n",
            "Epoch [433/10000], Loss: 3.2049\n",
            "Epoch [434/10000], Loss: 2.8145\n",
            "Epoch [435/10000], Loss: 3.2560\n",
            "Epoch [436/10000], Loss: 3.4033\n",
            "Epoch [437/10000], Loss: 3.6453\n",
            "Epoch [438/10000], Loss: 3.3633\n",
            "Epoch [439/10000], Loss: 2.9218\n",
            "Epoch [440/10000], Loss: 2.5362\n",
            "Epoch [441/10000], Loss: 3.5388\n",
            "Epoch [442/10000], Loss: 2.6695\n",
            "Epoch [443/10000], Loss: 3.8653\n",
            "Epoch [444/10000], Loss: 3.3390\n",
            "Epoch [445/10000], Loss: 3.8160\n",
            "Epoch [446/10000], Loss: 3.3805\n",
            "Epoch [447/10000], Loss: 3.4822\n",
            "Epoch [448/10000], Loss: 3.5245\n",
            "Epoch [449/10000], Loss: 3.2864\n",
            "Epoch [450/10000], Loss: 3.5721\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [451/10000], Loss: 4.6975\n",
            "Epoch [452/10000], Loss: 3.4673\n",
            "Epoch [453/10000], Loss: 2.9908\n",
            "Epoch [454/10000], Loss: 3.5757\n",
            "Epoch [455/10000], Loss: 2.7913\n",
            "Epoch [456/10000], Loss: 3.5259\n",
            "Epoch [457/10000], Loss: 2.9649\n",
            "Epoch [458/10000], Loss: 3.1542\n",
            "Epoch [459/10000], Loss: 3.2812\n",
            "Epoch [460/10000], Loss: 2.6390\n",
            "Epoch [461/10000], Loss: 4.2516\n",
            "Epoch [462/10000], Loss: 3.0630\n",
            "Epoch [463/10000], Loss: 3.1781\n",
            "Epoch [464/10000], Loss: 3.3235\n",
            "Epoch [465/10000], Loss: 3.1640\n",
            "Epoch [466/10000], Loss: 3.5293\n",
            "Epoch [467/10000], Loss: 2.7183\n",
            "Epoch [468/10000], Loss: 2.8739\n",
            "Epoch [469/10000], Loss: 3.4054\n",
            "Epoch [470/10000], Loss: 3.9413\n",
            "Epoch [471/10000], Loss: 3.0243\n",
            "Epoch [472/10000], Loss: 3.8511\n",
            "Epoch [473/10000], Loss: 3.0434\n",
            "Epoch [474/10000], Loss: 3.5093\n",
            "Epoch [475/10000], Loss: 3.7401\n",
            "Epoch [476/10000], Loss: 3.0723\n",
            "Epoch [477/10000], Loss: 2.4972\n",
            "Epoch [478/10000], Loss: 3.1857\n",
            "Epoch [479/10000], Loss: 3.7639\n",
            "Epoch [480/10000], Loss: 2.7317\n",
            "Epoch [481/10000], Loss: 3.7169\n",
            "Epoch [482/10000], Loss: 3.6847\n",
            "Epoch [483/10000], Loss: 3.1838\n",
            "Epoch [484/10000], Loss: 2.7322\n",
            "Epoch [485/10000], Loss: 2.5415\n",
            "Epoch [486/10000], Loss: 3.0185\n",
            "Epoch [487/10000], Loss: 3.4939\n",
            "Epoch [488/10000], Loss: 2.5932\n",
            "Epoch [489/10000], Loss: 3.2809\n",
            "Epoch [490/10000], Loss: 2.9520\n",
            "Epoch [491/10000], Loss: 3.2512\n",
            "Epoch [492/10000], Loss: 3.5208\n",
            "Epoch [493/10000], Loss: 3.5498\n",
            "Epoch [494/10000], Loss: 3.1656\n",
            "Epoch [495/10000], Loss: 3.6628\n",
            "Epoch [496/10000], Loss: 3.0057\n",
            "Epoch [497/10000], Loss: 3.4047\n",
            "Epoch [498/10000], Loss: 3.8340\n",
            "Epoch [499/10000], Loss: 2.8813\n",
            "Epoch [500/10000], Loss: 3.5047\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [501/10000], Loss: 3.0973\n",
            "Epoch [502/10000], Loss: 3.0919\n",
            "Epoch [503/10000], Loss: 2.8175\n",
            "Epoch [504/10000], Loss: 3.1005\n",
            "Epoch [505/10000], Loss: 4.0617\n",
            "Epoch [506/10000], Loss: 3.4285\n",
            "Epoch [507/10000], Loss: 3.2367\n",
            "Epoch [508/10000], Loss: 2.5715\n",
            "Epoch [509/10000], Loss: 3.6790\n",
            "Epoch [510/10000], Loss: 3.9483\n",
            "Epoch [511/10000], Loss: 3.3709\n",
            "Epoch [512/10000], Loss: 3.4471\n",
            "Epoch [513/10000], Loss: 3.1177\n",
            "Epoch [514/10000], Loss: 3.5021\n",
            "Epoch [515/10000], Loss: 3.4031\n",
            "Epoch [516/10000], Loss: 2.8036\n",
            "Epoch [517/10000], Loss: 2.8974\n",
            "Epoch [518/10000], Loss: 2.7498\n",
            "Epoch [519/10000], Loss: 3.9133\n",
            "Epoch [520/10000], Loss: 3.6284\n",
            "Epoch [521/10000], Loss: 2.7451\n",
            "Epoch [522/10000], Loss: 3.9821\n",
            "Epoch [523/10000], Loss: 3.9058\n",
            "Epoch [524/10000], Loss: 3.1435\n",
            "Epoch [525/10000], Loss: 3.6349\n",
            "Epoch [526/10000], Loss: 3.6855\n",
            "Epoch [527/10000], Loss: 2.8477\n",
            "Epoch [528/10000], Loss: 3.3157\n",
            "Epoch [529/10000], Loss: 2.8917\n",
            "Epoch [530/10000], Loss: 2.8226\n",
            "Epoch [531/10000], Loss: 3.1045\n",
            "Epoch [532/10000], Loss: 3.6782\n",
            "Epoch [533/10000], Loss: 3.2348\n",
            "Epoch [534/10000], Loss: 3.1976\n",
            "Epoch [535/10000], Loss: 3.4587\n",
            "Epoch [536/10000], Loss: 3.0748\n",
            "Epoch [537/10000], Loss: 3.7410\n",
            "Epoch [538/10000], Loss: 3.6308\n",
            "Epoch [539/10000], Loss: 3.8014\n",
            "Epoch [540/10000], Loss: 2.6119\n",
            "Epoch [541/10000], Loss: 2.9297\n",
            "Epoch [542/10000], Loss: 3.4460\n",
            "Epoch [543/10000], Loss: 2.6099\n",
            "Epoch [544/10000], Loss: 2.8163\n",
            "Epoch [545/10000], Loss: 3.3505\n",
            "Epoch [546/10000], Loss: 3.2522\n",
            "Epoch [547/10000], Loss: 3.3856\n",
            "Epoch [548/10000], Loss: 3.3381\n",
            "Epoch [549/10000], Loss: 3.0703\n",
            "Epoch [550/10000], Loss: 2.9372\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [551/10000], Loss: 2.9194\n",
            "Epoch [552/10000], Loss: 3.0996\n",
            "Epoch [553/10000], Loss: 2.7490\n",
            "Epoch [554/10000], Loss: 2.9005\n",
            "Epoch [555/10000], Loss: 2.5495\n",
            "Epoch [556/10000], Loss: 2.8537\n",
            "Epoch [557/10000], Loss: 2.5775\n",
            "Epoch [558/10000], Loss: 2.9610\n",
            "Epoch [559/10000], Loss: 3.0340\n",
            "Epoch [560/10000], Loss: 3.2165\n",
            "Epoch [561/10000], Loss: 3.5469\n",
            "Epoch [562/10000], Loss: 4.2462\n",
            "Epoch [563/10000], Loss: 2.5864\n",
            "Epoch [564/10000], Loss: 3.2692\n",
            "Epoch [565/10000], Loss: 3.4631\n",
            "Epoch [566/10000], Loss: 3.3237\n",
            "Epoch [567/10000], Loss: 2.9172\n",
            "Epoch [568/10000], Loss: 3.3856\n",
            "Epoch [569/10000], Loss: 3.2759\n",
            "Epoch [570/10000], Loss: 3.3202\n",
            "Epoch [571/10000], Loss: 3.3934\n",
            "Epoch [572/10000], Loss: 3.7908\n",
            "Epoch [573/10000], Loss: 2.9889\n",
            "Epoch [574/10000], Loss: 3.6973\n",
            "Epoch [575/10000], Loss: 3.0701\n",
            "Epoch [576/10000], Loss: 2.7902\n",
            "Epoch [577/10000], Loss: 3.4611\n",
            "Epoch [578/10000], Loss: 2.7349\n",
            "Epoch [579/10000], Loss: 3.4126\n",
            "Epoch [580/10000], Loss: 3.0175\n",
            "Epoch [581/10000], Loss: 3.0171\n",
            "Epoch [582/10000], Loss: 3.8085\n",
            "Epoch [583/10000], Loss: 2.7032\n",
            "Epoch [584/10000], Loss: 3.6809\n",
            "Epoch [585/10000], Loss: 3.6545\n",
            "Epoch [586/10000], Loss: 2.8920\n",
            "Epoch [587/10000], Loss: 2.8678\n",
            "Epoch [588/10000], Loss: 3.4091\n",
            "Epoch [589/10000], Loss: 3.2010\n",
            "Epoch [590/10000], Loss: 2.8682\n",
            "Epoch [591/10000], Loss: 2.8238\n",
            "Epoch [592/10000], Loss: 3.1730\n",
            "Epoch [593/10000], Loss: 3.8210\n",
            "Epoch [594/10000], Loss: 3.5423\n",
            "Epoch [595/10000], Loss: 3.2683\n",
            "Epoch [596/10000], Loss: 3.5243\n",
            "Epoch [597/10000], Loss: 3.3913\n",
            "Epoch [598/10000], Loss: 3.4969\n",
            "Epoch [599/10000], Loss: 3.4710\n",
            "Epoch [600/10000], Loss: 3.2439\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [601/10000], Loss: 2.8000\n",
            "Epoch [602/10000], Loss: 3.0683\n",
            "Epoch [603/10000], Loss: 3.3377\n",
            "Epoch [604/10000], Loss: 3.0539\n",
            "Epoch [605/10000], Loss: 4.1855\n",
            "Epoch [606/10000], Loss: 2.3354\n",
            "Epoch [607/10000], Loss: 3.6839\n",
            "Epoch [608/10000], Loss: 3.0896\n",
            "Epoch [609/10000], Loss: 3.5900\n",
            "Epoch [610/10000], Loss: 3.3360\n",
            "Epoch [611/10000], Loss: 2.9166\n",
            "Epoch [612/10000], Loss: 3.5994\n",
            "Epoch [613/10000], Loss: 2.9611\n",
            "Epoch [614/10000], Loss: 3.6414\n",
            "Epoch [615/10000], Loss: 3.3429\n",
            "Epoch [616/10000], Loss: 2.7611\n",
            "Epoch [617/10000], Loss: 3.2182\n",
            "Epoch [618/10000], Loss: 3.2260\n",
            "Epoch [619/10000], Loss: 3.2002\n",
            "Epoch [620/10000], Loss: 3.4024\n",
            "Epoch [621/10000], Loss: 4.1390\n",
            "Epoch [622/10000], Loss: 3.0178\n",
            "Epoch [623/10000], Loss: 3.6900\n",
            "Epoch [624/10000], Loss: 2.2475\n",
            "Epoch [625/10000], Loss: 3.1644\n",
            "Epoch [626/10000], Loss: 3.0993\n",
            "Epoch [627/10000], Loss: 3.0952\n",
            "Epoch [628/10000], Loss: 3.1373\n",
            "Epoch [629/10000], Loss: 3.7664\n",
            "Epoch [630/10000], Loss: 3.8041\n",
            "Epoch [631/10000], Loss: 4.3015\n",
            "Epoch [632/10000], Loss: 2.7968\n",
            "Epoch [633/10000], Loss: 4.2964\n",
            "Epoch [634/10000], Loss: 3.1323\n",
            "Epoch [635/10000], Loss: 3.4198\n",
            "Epoch [636/10000], Loss: 3.3868\n",
            "Epoch [637/10000], Loss: 3.9312\n",
            "Epoch [638/10000], Loss: 3.1576\n",
            "Epoch [639/10000], Loss: 3.4129\n",
            "Epoch [640/10000], Loss: 4.3067\n",
            "Epoch [641/10000], Loss: 3.2370\n",
            "Epoch [642/10000], Loss: 3.4142\n",
            "Epoch [643/10000], Loss: 3.3274\n",
            "Epoch [644/10000], Loss: 3.1562\n",
            "Epoch [645/10000], Loss: 3.2013\n",
            "Epoch [646/10000], Loss: 3.0386\n",
            "Epoch [647/10000], Loss: 2.9920\n",
            "Epoch [648/10000], Loss: 3.4281\n",
            "Epoch [649/10000], Loss: 3.3973\n",
            "Epoch [650/10000], Loss: 2.9531\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [651/10000], Loss: 3.0524\n",
            "Epoch [652/10000], Loss: 3.7090\n",
            "Epoch [653/10000], Loss: 3.0756\n",
            "Epoch [654/10000], Loss: 3.3097\n",
            "Epoch [655/10000], Loss: 2.6234\n",
            "Epoch [656/10000], Loss: 3.4759\n",
            "Epoch [657/10000], Loss: 3.8761\n",
            "Epoch [658/10000], Loss: 3.7680\n",
            "Epoch [659/10000], Loss: 2.9791\n",
            "Epoch [660/10000], Loss: 3.2592\n",
            "Epoch [661/10000], Loss: 3.3104\n",
            "Epoch [662/10000], Loss: 3.6917\n",
            "Epoch [663/10000], Loss: 4.0276\n",
            "Epoch [664/10000], Loss: 2.7130\n",
            "Epoch [665/10000], Loss: 2.8209\n",
            "Epoch [666/10000], Loss: 3.6420\n",
            "Epoch [667/10000], Loss: 3.2727\n",
            "Epoch [668/10000], Loss: 3.0936\n",
            "Epoch [669/10000], Loss: 2.4919\n",
            "Epoch [670/10000], Loss: 3.4042\n",
            "Epoch [671/10000], Loss: 2.8230\n",
            "Epoch [672/10000], Loss: 2.8322\n",
            "Epoch [673/10000], Loss: 2.8755\n",
            "Epoch [674/10000], Loss: 3.3602\n",
            "Epoch [675/10000], Loss: 3.1673\n",
            "Epoch [676/10000], Loss: 3.3482\n",
            "Epoch [677/10000], Loss: 3.3671\n",
            "Epoch [678/10000], Loss: 3.0697\n",
            "Epoch [679/10000], Loss: 3.8351\n",
            "Epoch [680/10000], Loss: 3.3031\n",
            "Epoch [681/10000], Loss: 3.3544\n",
            "Epoch [682/10000], Loss: 3.0777\n",
            "Epoch [683/10000], Loss: 2.9275\n",
            "Epoch [684/10000], Loss: 3.4841\n",
            "Epoch [685/10000], Loss: 3.7362\n",
            "Epoch [686/10000], Loss: 3.2672\n",
            "Epoch [687/10000], Loss: 3.3825\n",
            "Epoch [688/10000], Loss: 2.7172\n",
            "Epoch [689/10000], Loss: 3.6628\n",
            "Epoch [690/10000], Loss: 3.4961\n",
            "Epoch [691/10000], Loss: 3.4282\n",
            "Epoch [692/10000], Loss: 3.4466\n",
            "Epoch [693/10000], Loss: 2.9674\n",
            "Epoch [694/10000], Loss: 2.8402\n",
            "Epoch [695/10000], Loss: 2.6439\n",
            "Epoch [696/10000], Loss: 3.2012\n",
            "Epoch [697/10000], Loss: 3.5001\n",
            "Epoch [698/10000], Loss: 3.0603\n",
            "Epoch [699/10000], Loss: 2.7576\n",
            "Epoch [700/10000], Loss: 3.0746\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [701/10000], Loss: 3.4660\n",
            "Epoch [702/10000], Loss: 3.2279\n",
            "Epoch [703/10000], Loss: 2.7858\n",
            "Epoch [704/10000], Loss: 3.7859\n",
            "Epoch [705/10000], Loss: 3.0989\n",
            "Epoch [706/10000], Loss: 2.2749\n",
            "Epoch [707/10000], Loss: 2.9473\n",
            "Epoch [708/10000], Loss: 3.8217\n",
            "Epoch [709/10000], Loss: 3.5963\n",
            "Epoch [710/10000], Loss: 2.4975\n",
            "Epoch [711/10000], Loss: 2.6491\n",
            "Epoch [712/10000], Loss: 3.2704\n",
            "Epoch [713/10000], Loss: 2.8323\n",
            "Epoch [714/10000], Loss: 3.0193\n",
            "Epoch [715/10000], Loss: 4.3787\n",
            "Epoch [716/10000], Loss: 3.8022\n",
            "Epoch [717/10000], Loss: 3.1566\n",
            "Epoch [718/10000], Loss: 3.8311\n",
            "Epoch [719/10000], Loss: 3.1353\n",
            "Epoch [720/10000], Loss: 3.1888\n",
            "Epoch [721/10000], Loss: 2.8350\n",
            "Epoch [722/10000], Loss: 3.0473\n",
            "Epoch [723/10000], Loss: 3.8170\n",
            "Epoch [724/10000], Loss: 3.3278\n",
            "Epoch [725/10000], Loss: 2.8605\n",
            "Epoch [726/10000], Loss: 3.0919\n",
            "Epoch [727/10000], Loss: 2.4418\n",
            "Epoch [728/10000], Loss: 3.5112\n",
            "Epoch [729/10000], Loss: 3.8179\n",
            "Epoch [730/10000], Loss: 3.9870\n",
            "Epoch [731/10000], Loss: 3.2642\n",
            "Epoch [732/10000], Loss: 3.8579\n",
            "Epoch [733/10000], Loss: 3.0417\n",
            "Epoch [734/10000], Loss: 2.3472\n",
            "Epoch [735/10000], Loss: 2.6678\n",
            "Epoch [736/10000], Loss: 3.2571\n",
            "Epoch [737/10000], Loss: 2.7944\n",
            "Epoch [738/10000], Loss: 3.5861\n",
            "Epoch [739/10000], Loss: 2.8532\n",
            "Epoch [740/10000], Loss: 3.3601\n",
            "Epoch [741/10000], Loss: 3.1643\n",
            "Epoch [742/10000], Loss: 4.1201\n",
            "Epoch [743/10000], Loss: 3.0569\n",
            "Epoch [744/10000], Loss: 2.6441\n",
            "Epoch [745/10000], Loss: 3.6353\n",
            "Epoch [746/10000], Loss: 2.4498\n",
            "Epoch [747/10000], Loss: 2.7518\n",
            "Epoch [748/10000], Loss: 2.9143\n",
            "Epoch [749/10000], Loss: 3.0305\n",
            "Epoch [750/10000], Loss: 3.6830\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [751/10000], Loss: 3.6590\n",
            "Epoch [752/10000], Loss: 2.8983\n",
            "Epoch [753/10000], Loss: 3.4304\n",
            "Epoch [754/10000], Loss: 2.3210\n",
            "Epoch [755/10000], Loss: 3.7983\n",
            "Epoch [756/10000], Loss: 3.2193\n",
            "Epoch [757/10000], Loss: 2.7816\n",
            "Epoch [758/10000], Loss: 3.3308\n",
            "Epoch [759/10000], Loss: 2.4790\n",
            "Epoch [760/10000], Loss: 4.1217\n",
            "Epoch [761/10000], Loss: 3.4619\n",
            "Epoch [762/10000], Loss: 3.6708\n",
            "Epoch [763/10000], Loss: 2.7573\n",
            "Epoch [764/10000], Loss: 3.0864\n",
            "Epoch [765/10000], Loss: 3.5001\n",
            "Epoch [766/10000], Loss: 2.7922\n",
            "Epoch [767/10000], Loss: 3.1130\n",
            "Epoch [768/10000], Loss: 3.0344\n",
            "Epoch [769/10000], Loss: 3.0342\n",
            "Epoch [770/10000], Loss: 3.4793\n",
            "Epoch [771/10000], Loss: 2.9535\n",
            "Epoch [772/10000], Loss: 2.7106\n",
            "Epoch [773/10000], Loss: 3.2768\n",
            "Epoch [774/10000], Loss: 3.5498\n",
            "Epoch [775/10000], Loss: 3.0824\n",
            "Epoch [776/10000], Loss: 3.6239\n",
            "Epoch [777/10000], Loss: 3.1711\n",
            "Epoch [778/10000], Loss: 2.4503\n",
            "Epoch [779/10000], Loss: 2.6097\n",
            "Epoch [780/10000], Loss: 3.6420\n",
            "Epoch [781/10000], Loss: 3.3211\n",
            "Epoch [782/10000], Loss: 3.1951\n",
            "Epoch [783/10000], Loss: 3.5285\n",
            "Epoch [784/10000], Loss: 2.8074\n",
            "Epoch [785/10000], Loss: 3.1525\n",
            "Epoch [786/10000], Loss: 2.8561\n",
            "Epoch [787/10000], Loss: 3.1393\n",
            "Epoch [788/10000], Loss: 3.4907\n",
            "Epoch [789/10000], Loss: 3.2417\n",
            "Epoch [790/10000], Loss: 2.4846\n",
            "Epoch [791/10000], Loss: 3.1619\n",
            "Epoch [792/10000], Loss: 3.3134\n",
            "Epoch [793/10000], Loss: 3.1665\n",
            "Epoch [794/10000], Loss: 2.1511\n",
            "Epoch [795/10000], Loss: 3.1205\n",
            "Epoch [796/10000], Loss: 3.0884\n",
            "Epoch [797/10000], Loss: 3.3849\n",
            "Epoch [798/10000], Loss: 3.8467\n",
            "Epoch [799/10000], Loss: 3.8652\n",
            "Epoch [800/10000], Loss: 3.0832\n",
            "saving checkpoint to ./drive/MyDrive/MLchess/checkpoint.pt\n",
            "Epoch [801/10000], Loss: 3.0895\n",
            "Epoch [802/10000], Loss: 2.9020\n",
            "Epoch [803/10000], Loss: 3.3588\n",
            "Epoch [804/10000], Loss: 2.8232\n",
            "Epoch [805/10000], Loss: 2.9953\n",
            "Epoch [806/10000], Loss: 3.4851\n",
            "Epoch [807/10000], Loss: 3.5973\n",
            "Epoch [808/10000], Loss: 2.5441\n",
            "Epoch [809/10000], Loss: 4.0509\n",
            "Epoch [810/10000], Loss: 3.1137\n",
            "Epoch [811/10000], Loss: 3.0611\n",
            "Epoch [812/10000], Loss: 2.7862\n",
            "Epoch [813/10000], Loss: 2.8998\n",
            "Epoch [814/10000], Loss: 3.1187\n",
            "Epoch [815/10000], Loss: 3.4830\n",
            "Epoch [816/10000], Loss: 2.6096\n",
            "Epoch [817/10000], Loss: 3.2839\n",
            "Epoch [818/10000], Loss: 2.8739\n",
            "Epoch [819/10000], Loss: 3.0966\n",
            "Epoch [820/10000], Loss: 2.1140\n",
            "Epoch [821/10000], Loss: 2.9794\n",
            "Epoch [822/10000], Loss: 3.0371\n",
            "Epoch [823/10000], Loss: 3.4169\n",
            "Epoch [824/10000], Loss: 3.4735\n",
            "Epoch [825/10000], Loss: 3.1671\n",
            "Epoch [826/10000], Loss: 3.0000\n",
            "Epoch [827/10000], Loss: 3.2433\n",
            "Epoch [828/10000], Loss: 3.2210\n",
            "Epoch [829/10000], Loss: 3.1270\n",
            "Epoch [830/10000], Loss: 3.0364\n",
            "Epoch [831/10000], Loss: 3.5383\n",
            "Epoch [832/10000], Loss: 3.2288\n",
            "Epoch [833/10000], Loss: 3.1183\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-67a39429d1f0>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_dest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtargets_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "num_epochs = 10_000\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "dataset = TensorDataset(input_batch, output_src_batch, output_dst_batch)\n",
        "dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets_source, targets_dest in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets_source = targets_source.to(device)\n",
        "        targets_dest = targets_dest.to(device)\n",
        "    \n",
        "        # Forward pass\n",
        "        outputs_source, outputs_dest = model(inputs)\n",
        "    \n",
        "        loss_source = loss_fn(outputs_source, targets_source)\n",
        "        loss_dest = loss_fn(outputs_dest, targets_dest)\n",
        "    \n",
        "        # Compute total loss\n",
        "        loss = loss_source + loss_dest\n",
        "    \n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()       \n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        filename = './drive/MyDrive/MLchess/checkpoint.pt'\n",
        "        print(f'saving checkpoint to {filename}')\n",
        "        torch.save(model.state_dict(), filename)  # Save the model checkpoint\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AmJ0WPsdzz6"
      },
      "source": [
        "# Spot check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkeZIghHd16P",
        "outputId": "623e17ba-e5bf-4f22-8d1f-8e1f04b040ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1], device='cuda:0')\n",
            "D\n"
          ]
        }
      ],
      "source": [
        "gamestr = \"\"\"[Event \"Rated Bullet game\"]\n",
        "[Site \"https://lichess.org/G60KFAT0\"]\n",
        "[Date \"2023.05.22\"]\n",
        "[White \"smt2\"]\n",
        "[Black \"salahd4d5\"]\n",
        "[Result \"0-1\"]\n",
        "[UTCDate \"2023.05.22\"]\n",
        "[UTCTime \"00:52:33\"]\n",
        "[WhiteElo \"1391\"]\n",
        "[BlackElo \"1452\"]\n",
        "[WhiteRatingDiff \"-5\"]\n",
        "[BlackRatingDiff \"+30\"]\n",
        "[Variant \"Standard\"]\n",
        "[TimeControl \"120+1\"]\n",
        "[ECO \"C50\"]\n",
        "[Opening \"Italian Game: Giuoco Pianissimo, Normal\"]\n",
        "[Termination \"Normal\"]\n",
        "[Annotator \"lichess.org\"]\n",
        "\n",
        "1. e4 e5 2. Nf3 Nc6 3. Bc4 Bc5 4. d3 Nf6 { C50 Italian Game: Giuoco Pianissimo, Normal } 5. Be3 Bxe3 6. fxe3 O-O 7. O-O d6 8. Qd2 Be6 9. Na3 Ng4 10. h3 Nf6 11. Rf2 Bd7 12. Raf1 Ne7 13. Ng5 h6 14. Nf3 Ng6 15. Nh2 a5 16. Qe2 c6 17. Rf3 b5 18. Bb3 a4 19. Bxf7+ Rxf7 20. Rg3 Nh4 21. Ng4 Nxg4 22. Rxf7 Kxf7 23. Rxg4 Bxg4 24. Qxg4 Qf6 25. g3 Nf3+ 26. Qxf3 Qxf3 27. Nxb5 cxb5 28. d4 Qxe3+ 29. Kf1 Qxd4 30. c3 Qd1+ 31. Kf2 Ke6 32. g4 Rf8+ 33. Kg3 Qg1+ 34. Kh4 g6 35. g5 Qxg5# { Black wins by checkmate. } 0-1\"\"\"\n",
        "\n",
        "from chess import pgn\n",
        "from io import StringIO\n",
        "game = pgn.read_game(StringIO(gamestr))\n",
        "input = [list(map(one_input, map(str, game.mainline_moves())))]\n",
        "padding = torch.zeros((1, 128))  # A tensor of zeros with the same width as your one-hot encoded representations\n",
        "\n",
        "input = [\n",
        "    torch.cat([torch.stack(seq), padding.repeat(max_num_moves - len(seq), 1)], dim=0)\n",
        "    for seq in input]\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    input = torch.stack(input)\n",
        "    input = input.to(device)\n",
        "    \n",
        "    output = model(input)\n",
        "    probabilities = torch.nn.functional.softmax(output, dim=-1)\n",
        "\n",
        "    # Use argmax to get the most likely class\n",
        "    predicted_class = probabilities.argmax(dim=-1)\n",
        "    print(predicted_class)\n",
        "    match predicted_class:\n",
        "        case 0:\n",
        "            print(\"W\")\n",
        "        case 1:\n",
        "            print(\"D\")\n",
        "        case 2:\n",
        "            print(\"B\")\n",
        "        case _:\n",
        "            print(predicted_class)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWf7lNOIK7wl"
      },
      "source": [
        "# Verification Using Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R7U9-BYLmca",
        "outputId": "2be05c13-80d3-4354-dbab-55c1b9ed5fb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "try:\n",
        "    del dataset\n",
        "    del dataloader\n",
        "    del input_batch\n",
        "    del output_batch\n",
        "except NameError:\n",
        "    pass\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "input = test_input.to(device)\n",
        "output = model(input)\n",
        "loss_fn(output, test_output.to(device))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6BFfd4EXlK8"
      },
      "source": [
        "# Play with AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "exPp2zmWXmm3",
        "outputId": "0c50b68f-79cc-4549-c335-395faaec7a9d"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.2\" baseProfile=\"tiny\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><desc><pre>r n b q k b n r\np p p p p p p p\n. . . . . . . .\n. . . . . . . .\n. . . . P . . .\n. . . . . . . .\nP P P P . P P P\nR N B Q K B N R</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#000\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-bishop\" class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-queen\" class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"0\" y=\"0\" width=\"390\" height=\"390\" fill=\"#212121\" /><g transform=\"translate(20, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light lastmove e2\" stroke=\"none\" fill=\"#cdd16a\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light lastmove e4\" stroke=\"none\" fill=\"#cdd16a\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(60, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(105, 330)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(150, 330)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(195, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(240, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(285, 330)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(330, 330)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(60, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(195, 195)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(15, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(60, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(150, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(195, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(240, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(285, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(330, 60)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(15, 15)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(60, 15)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(105, 15)\" /><use href=\"#black-queen\" xlink:href=\"#black-queen\" transform=\"translate(150, 15)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(195, 15)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(240, 15)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(285, 15)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(330, 15)\" /></svg>",
            "text/plain": [
              "Board('rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI's turn\n",
            "e2e4\n"
          ]
        },
        {
          "ename": "IllegalMoveError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalMoveError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-5e1e9499ca69>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Game over\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0minteractive_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-115-5e1e9499ca69>\u001b[0m in \u001b[0;36minteractive_game\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Make the move on the board\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_san\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mchess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chess/__init__.py\u001b[0m in \u001b[0;36mpush_san\u001b[0;34m(self, san)\u001b[0m\n\u001b[1;32m   3077\u001b[0m                 \u001b[0;34m-\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mAmbiguousMoveError\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSAN\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mambiguous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m         \"\"\"\n\u001b[0;32m-> 3079\u001b[0;31m         \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_san\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3080\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chess/__init__.py\u001b[0m in \u001b[0;36mparse_san\u001b[0;34m(self, san)\u001b[0m\n\u001b[1;32m   3036\u001b[0m             \u001b[0;31m# Allow fully specified moves, even if they are not pawn moves,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m             \u001b[0;31m# including castling moves.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3038\u001b[0;31m             \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_square\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpromotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpromotion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpromotion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chess/__init__.py\u001b[0m in \u001b[0;36mfind_move\u001b[0;34m(self, from_square, to_square, promotion)\u001b[0m\n\u001b[1;32m   2331\u001b[0m         \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_chess960\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchess960\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_square\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_square\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpromotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_legal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2333\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIllegalMoveError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"no matching legal move for {move.uci()} ({SQUARE_NAMES[from_square]} -> {SQUARE_NAMES[to_square]}) in {self.fen()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalMoveError\u001b[0m: no matching legal move for e2e4 (e2 -> e4) in rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1"
          ]
        }
      ],
      "source": [
        "import chess\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "def output_to_move(source_output, dest_output):\n",
        "    source_square = torch.argmax(source_output).item()\n",
        "    dest_square = torch.argmax(dest_output).item()\n",
        "    return reverse_cache[source_square] + reverse_cache[dest_square]\n",
        "\n",
        "\n",
        "def interactive_game():\n",
        "    board = chess.Board()\n",
        "\n",
        "    while not board.is_game_over():\n",
        "        clear_output(wait=True)\n",
        "        display(board)\n",
        "\n",
        "        \n",
        "\n",
        "        if board.turn == chess.WHITE:\n",
        "            # Get user input\n",
        "            move = \"e2e4\"\n",
        "        else:\n",
        "            # AI's turn\n",
        "            print(\"AI's turn\")\n",
        "\n",
        "            # Convert board state to model input\n",
        "            input_tensor = board_to_tensor(board)\n",
        "        \n",
        "            # Get the move from the model\n",
        "            source_output, dest_output = model(input_tensor.to(device))\n",
        "        \n",
        "            # Convert model output to move\n",
        "            move = output_to_move(source_output, dest_output)\n",
        "            print(move)\n",
        "        \n",
        "        # Make the move on the board\n",
        "        board.push_san(move)\n",
        "\n",
        "        if board.turn == chess.WHITE:\n",
        "            break\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    display(board)\n",
        "    print(\"Game over\")\n",
        "\n",
        "interactive_game()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1E0nAwpzQp-1WtDdyaT1I1Sg5AfPKbWS2",
      "authorship_tag": "ABX9TyMm3XUW63MSU4pK1OBrgP5H",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}